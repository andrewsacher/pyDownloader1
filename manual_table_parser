import os
import requests
import urllib
import math
import pandas as pd
import numpy as np
from bs4 import BeautifulSoup 

class web_data(object):
    
    def __init__(self, url):
        
        self.url      = url
        self.r        = requests.get(self.url)
        self.raw_html = self.r.text
        self.url_soup = BeautifulSoup(self.raw_html)

    #=======================================
    # Method to download data files to disk
    #=======================================
    
    def download_files(self, 
                       save_path, 
                       ext):
       
        self.save_path = save_path
        self.ext = ext
         
        # Get links to files
        self.links = list()
        for link in self.url_soup.find_all("a"):
            this_link = link.get("href")
            if not this_link == None: 
                if this_link.find("." + self.ext) >= 0:
                    self.links.append(urllib.request.urljoin(self.url, this_link))
        
        # Download files to directory
        self.failed_links = list()
        for link in self.links:
            file_path = os.path.join(self.save_path, link[link.rfind("/") + 1: ])
            try:
                urllib.request.urlretrieve(link, file_path)
            except:
                self.failed_links.append(link)
        
        if len(self.failed_links) > 0:
            print("The following files could not be downloaded:")
            for s in self.failed_links:
                print(s)
                
                
                
                
                

table = soup.find_all("table")[0]

# Find number of rows and columns
n_cols = 0
n_rows = 0

for row in table.find_all("tr"):
    col_tags = row.find_all(["td", "th"])
    if len(col_tags) > 0:
        n_rows += 1
        if len(col_tags) > n_cols:
            n_cols = len(col_tags)
            
#================
# FILL DATAFRAME
#================

# Create dataframe
df = pd.DataFrame(index = range(0, n_rows), columns = range(0, n_cols))

# Start by iterating over each row in this table...
row_counter = 0
skip_index = [0 for i in range(0, n_cols)]

for row in table.find_all("tr"):

    
    # Skip row if its blank
    if len(row.find_all(["td", "th"])) == 0:
        next
    
    else:
        
        # Get all cells containing data in this row
        columns = row.find_all(["td", "th"])
        this_skip_index = copy.deepcopy(skip_index)
        col_counter = -1 
        col_dim = []
        row_dim = []
        col_dim_counter = -1
        row_dim_counter = -1
        for col in columns:
                
            # Determine cell dimensions
            colspan = col.get("colspan")
            if colspan is None:
                col_dim.append(1)
            else:
                col_dim.append(int(colspan))
            col_dim_counter += 1
                
            rowspan = col.get("rowspan")
            if rowspan is None:
                row_dim.append(1)
            else:
                row_dim.append(int(rowspan))
            row_dim_counter += 1
                
            # Adjust column counter
            col_counter = col_counter + col_dim[col_dim_counter - 1]
            while skip_index[col_counter] > 0:
                col_counter += 1
            
            if row_counter < 4:
                print(str(row_counter) + ", " + str(col_counter))
                print(skip_index)
                
            # Parse out footnotes
            cell_data = col.get_text()
            footnote = col.find(["sup", "sub"])
            if footnote is not None:
                cell_data = cell_data[0: -len(footnote.get_text())]
            
            # Insert data into cell
            try:   
                df.iat[row_counter, col_counter] = cell_data
            except:
                print("failed")
            
            # Record column skipping index
            if row_dim[row_dim_counter] > 1:
                this_skip_index[col_counter] = row_dim[row_dim_counter]
    
    # Adjust row counter 
    row_counter += 1
    
    # Adjust column skipping index
    skip_index = [i - 1 if i > 0 else i for i in this_skip_index]
        
url = "https://www.ssa.gov/policy/docs/statcomps/supplement/2015/7a.html#table7.a2"

save_path = "C:\\Users\\jricco\\Downloads\\" 
df.to_csv(os.path.join(save_path + "test111.csv"))




